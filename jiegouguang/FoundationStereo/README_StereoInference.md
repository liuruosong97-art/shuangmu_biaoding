# StereoInference - FoundationStereo Large Model Wrapper

A production-ready interface to FoundationStereo's **3.1GB ViT-Large model** for stereo vision tasks, with automatic model detection and optimized configuration handling.

## ðŸš€ Large Model Features

- **Auto-Detection**: Automatically finds and loads the 3.1GB external model
- **Smart Configuration**: Handles missing `vit_size` parameters in config files
- **Memory Optimized**: Efficient loading and inference for large-scale models
- **Device Management**: Auto-detects CUDA/CPU with optimal settings
- **Production Ready**: Robust error handling and validation

## Quick Start

```python
from stereo_inference import StereoInference

# ðŸŽ¯ Auto-detect large model (recommended)
stereo_infer = StereoInference()  # Automatically finds 3.1GB model

# ðŸ“· Run inference
results = stereo_infer.infer(left_bgr, right_bgr, K_rect, baseline_m)
```

## Model Auto-Detection

The wrapper automatically detects models in this priority order:

1. **`/media/root123/DX/pretrained_models/23-51-11/model_best_bp2.pth`** (3.1GB external)
2. `./pretrained_models/23-51-11/model_best_bp2.pth` (local copy)

**Model Specifications:**
- **Size**: 3.1 GB ViT-Large model
- **Training**: 200,000 steps, 40 epochs
- **Architecture**: FoundationStereo with ViT-Large backbone
- **Max Disparity**: 416 pixels
- **Mixed Precision**: Enabled for memory efficiency

## Usage Patterns

### 1. Auto-Detection (Recommended)
```python
# Automatically finds the best available model
stereo_infer = StereoInference()
```

### 2. Explicit Model Path
```python
# Use specific model file
stereo_infer = StereoInference("/media/root123/DX/pretrained_models/23-51-11/model_best_bp2.pth")
```

### 3. Custom ViT Configuration
```python
# Specify ViT backbone size (vitl, vits, vitb, vitg)
stereo_infer = StereoInference(vit_size='vitl')
```

### 4. Device Control
```python
# Force specific device
stereo_infer = StereoInference(device='cuda')  # or 'cpu'
```

## Installation & Dependencies

Requires existing FoundationStereo environment:
```bash
conda activate foundation_stereo
pip install torch opencv-python numpy omegaconf
```

## API Reference

### StereoInference

#### `__init__(ckpt_path, device=None)`
- `ckpt_path`: Path to `.pth` model checkpoint
- `device`: Computing device (auto-detects CUDA if None)

#### `infer(left_img_bgr, right_img_bgr, K_rect, baseline_m, R1=None, to_original_left_cam=False, valid_iters=32)`

**Inputs:**
- `left_img_bgr`: Left rectified image (H,W,3) BGR uint8
- `right_img_bgr`: Right rectified image (H,W,3) BGR uint8
- `K_rect`: Rectified camera intrinsic matrix (3,3)
- `baseline_m`: Baseline distance in meters
- `R1`: Optional rotation matrix (3,3) from rectified to original left camera
- `to_original_left_cam`: If True and R1 provided, transform points to original frame
- `valid_iters`: Number of refinement iterations (default: 32)

**Returns:** Dictionary with:
- `disparity`: (H,W) disparity map in pixels
- `depth`: (H,W) depth map in meters (depth = fx * baseline / disparity)
- `points_cam1`: (H,W,3) point cloud in left camera coordinates (meters)
- `valid_mask`: (H,W) boolean mask for valid depth values (disparity > 0)

## Example Script Usage

```bash
python examples/run_stereo_infer.py \
    --left path/to/left.png \
    --right path/to/right.png \
    --intrinsic path/to/intrinsics.txt \
    --output ./output \
    --scale 0.5
```

**Intrinsics file format:**
```
394.837769 0.000000 321.689484 0.000000 395.248810 242.508881 0.000000 0.000000 1.000000
0.095150
```
Line 1: 9 values for 3x3 K matrix (row-major)
Line 2: Baseline in meters

**Outputs:**
- `disparity.png`: Colorized disparity visualization
- `depth.png`: Colorized depth visualization
- `depth_meter.npy`: Raw depth data (NumPy array)
- `pointcloud.ply`: 3D point cloud with RGB colors

## Testing

Run comprehensive unit tests:
```bash
python test_stereo_inference.py
```

**Test Coverage:**
- âœ… **Shape Tests**: Output dimensions match input dimensions
- âœ… **Scale Tests**: Depth formula `depth = fx*baseline/disparity` within 3% error
- âœ… **Rotation Tests**: Coordinate transformation consistency (`R @ R.T = I`)
- âœ… **Integration Tests**: End-to-end pipeline with real model weights

## Performance Characteristics

**Test Results (RealSense D435 IR images):**
- **Input Resolution**: 640Ã—480 â†’ 320Ã—240 (scale=0.5)
- **Valid Pixel Coverage**: 100.0% (76,800/76,800)
- **Depth Range**: 0.280 - 0.518m
- **Processing Time**: ~60 seconds (including model loading)
- **GPU Memory**: ~4GB (RTX 3080)

## Architecture Details

### Resolution Handling
- Network may output different resolution than input
- Automatically resizes disparity and scales values: `disp *= (W_input / W_output)`
- Preserves metric accuracy across scale changes

### Coordinate Systems
- **Default**: Points in rectified left camera coordinates
- **Optional**: Transform to original left camera via `R1.T @ points_rect`
- **Units**: All 3D coordinates in meters

### Depth Computation
```python
# Standard stereo formula
depth = fx * baseline_m / disparity
```

### Valid Mask
- `valid_mask = disparity > 0`
- Filters out invalid/occluded regions
- Used for point cloud generation

## File Structure

```
â”œâ”€â”€ stereo_inference.py           # Main StereoInference class
â”œâ”€â”€ test_stereo_inference.py      # Unit tests
â”œâ”€â”€ examples/
â”‚   â””â”€â”€ run_stereo_infer.py      # Complete usage example
â””â”€â”€ README.md                    # This file
```

## Validation Results

**Unit Tests**: 8/8 tests passed
- Shape consistency across resolutions âœ…
- Depth formula accuracy (fx*B/d) âœ…
- Coordinate transformation correctness âœ…
- Real model inference âœ…

**Real Data Test**:
- IR stereo pair from RealSense D435 âœ…
- 100% valid pixel coverage âœ…
- Depth range 0.28-0.52m (realistic for test scene) âœ…

## Implementation Notes

- Based on `scripts/run_demo.py` model loading logic
- RGB normalization to [0,1] range
- Automatic CUDA/CPU device selection
- Mixed precision inference support
- Input validation and error handling
- Memory-efficient point cloud generation

## Citation

If you use this wrapper, please cite the original FoundationStereo work:

```bibtex
@article{foundationstereo2024,
  title={FoundationStereo: ...},
  author={...},
  journal={...},
  year={2024}
}
```